\documentclass[12pt]{article}
\usepackage{natbib}
\usepackage{verbatim}
\usepackage{color}
\usepackage{url}
\usepackage{amsmath,graphicx,amssymb}
\bibpunct{(}{)}{;}{a}{}{,}
%\newcommand{\ignore}[1]{}

%\renewcommand{\baselinestretch}{1.7}
\setlength{\textheight}{9in}
\setlength{\textwidth}{6in}
\addtolength{\hoffset}{-0.45in}
\addtolength{\voffset}{-0.425in}

\newtheorem{thm}{Theorem}[section] %(If you want theorem numbered
\newtheorem{lemma}{Lemma}[section] %%    with section number.
\newtheorem{cor}{Corollary}[section]
\newtheorem{prop}{Proposition}[section]
\newtheorem{theorem}{Theorem}
\newcommand{\sgn}{\mathrm{sgn}}
\newcommand{\eps}{\epsilon}
\newcommand{\rarr}{\rightarrow}
\renewcommand{\b}{\mathbf b}
\newcommand{\qed}{{\unskip\nobreak\hfil\penalty50\hskip2em\vadjust{}
            \nobreak\hfil$\Box$\parfillskip=0pt\finalhyphendemerits=0\par}}
\newcommand{\ignore}[1]{}
\renewcommand{\thefootnote}{}

\newcommand{\yf}[1]{{\textcolor{red}{[Fan: #1]}}}
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%
%%%Last change on May 22 by Jin%%%%
%%%%%%%%%%%%%%%%%%%%%%

\begin{center}
{\bf \large Point-by-Point Response to Referee One}
\end{center}
Thank you very much for your careful reading our paper as well as your invaluable comments. We list our response to your comments as follows:\\ 
major comments\\

\noindent ``\textit {This paper is on manifold learning.  It is very confusing to read.''}\\
Answer:\\
To the best of our knowledge, manifold learning refers to an approach to non-linear dimensionality reduction, which is a method trying to find the new representation in a lower-dimensional space by approximately keeping the geometry properties of the input which resides in a high-dimensional ambient space. In this paper, we learn the original manifold in the ambient space. Therefore, our work is not on manifold learning.

\noindent ``\textit{For example, realistically the data generated by the manifold model are generated with noise, and the distribution and properties of the noise in conjunction with the reach of the manifold are critical for the recovery. However, these critical issues are bypassed by making strong assumptions whereby only the local behavior of ridges matters.''}\\
Answer:\\
We agree with the referee that the recovery is highly related with the noise and reach of the manifold. In our work, our model in Equation (1) on page assumes the data are generated as 
\begin{equation}\label{model}
x_i=\tilde{x}_i+\epsilon_i,
\end{equation}
where $\tilde{x}_i$ is the noiseless signal which belong to some manifold, and $\epsilon_i$ is the noise. We use the observations $x_i$ to recover the manifold. Therefore, we did not bypass consideration of the noise in our model.
\\
The reach is a global concept which describes the degree of curvature for the manifold. A larger {``\it reach''} indicates the samples resides further from the manifold can also has a unique projection onto $\cal M$. Therefore, the manifold is more flat globally.

\noindent ``\textit{ This approach may be interesting in its own rights, but the impact in the bigger picture is rather limited due to the limited perspective and the strong assumptions that are needed. The best part are the computational perspectives provided in the paper, while the theoretical developments are also not very interesting.'' }\\
Answer:\\
In fact, Assumption 2.2 is more likely to be thought as a ``Fact'' rather than an Assumption. The main intention of Assumption 2.2 is to say that any low-dimensional manifold can be thought as the ridge of some unknown density function. In this new version of the manuscript, we modify the presentation of Assumption 2.2 to avoid the confusion.
\\
\noindent ``\textit {A key problem is that the errors in the sampling of the data generated by the manifold are not modeled and the noise-contaminated data are not studied. The paper is not well written, the presentation is substandard  and is unfocused.'' }\\
Answer:\\
As in Equation \eqref{model}, we model the data as the noiseless $\tilde{x}_i\in \cal M$ plus the noise $\epsilon_i$. We consider the noise in the theories and all the cases of the numerical experiments.\\

\noindent ``\textit{Problematic assumptions include Assumption 2.2, where parts 1 and 3 are unclear and ambiguous and Assumption 3.4 which is equally unmotivated.''}\\
Answer:\\
1. Assumption 2.2 only wants to show that any noiseless manifold can be thought as the ridge of some background density function $p(x)$. And the method to construct $p(x)$ is not complicated.\\
2. Because of $\mu_s(x,r,h)\in(0,1),\mu_{s,t}(x,r,h)\in (0,1)$, we know 
\[
\mu(r,h) = \sup_x \max\{\max_s \mu_s(x,r,h), \max_{s,t}\mu_{s,t}(x,r,h)\}\in [0,1].
\]
Assumption 3.4 is a bit stronger by requiring $\mu(r,h)$ to be upper-bounded by a number $\ell$ which is less than 1 for all $h$.\\
 \noindent ``\textit{The simulations are too limited as there are many other competing methods to recover the manifolds that are considered, and these manifolds are too simple to be of much interest.'' } \\
 Answer:\\
 We compare four manifold fitting algorithms on the one-dimensional ring manifold to show the good recovery property of {\it l}-SCRE under the criteria of average margin and Hausdorff distance. The manifold fitting result does not rely on the dataset too much. The comparison results on the other dataset are similar. To justify, we add another numerical result corresponding to a 2-dimensional sphere in the 3D ambient space.

%\noindent ``\textit {1. My major concern is about the condition needed for the establishment of results in this paper.. The authors need $P^*$ to have constant eigengap, which is far too strong and not realistic (it implies $p_{ij}$ are in the constant order). Under this regime, the results the authors present are expected, not surprising, and more or less have been long known to theoretical researchers working on community detection. This limits the depth and scope of this paper. In community detection, a less trivial regime is when $p_{i,j}=o(1)$. A more interesting case is when $p_{ij}$ is around $\log n/n$. An exciting regime is when $p_{ij}=O(\log n/n)$, where the exact recovery is impossible. I think this paper is a good start and the authors are encouraged to continue along this path.}''\\
%Answer: \\
%Thanks for the important suggestion. We have researched a lot in this boundary of interest. In our new revision, we no longer require $p_{ij}$ are in the constant order, and have relaxed the condition to  $\max_{k_1,k_2}p_{i,j}=\varphi_n n^{-1}$ (see Assumption 4 in the revised paper), where $\varphi_n\to\infty$ and $\varphi_n\le n$ . It covers the regime $p_{ij}=O(\log n/n)$ as a special case. Our models and main theorems in the paper have been modified largely as condition of $p_{ij}$ changed.

%We also highlight that, in the revised paper, we have proved the strongly consistency of the proposed method with a better $l_\infty$-norm bound relative to SCORE and its variants, where the almost surely consistency is not provided there (or in other existing literature sources) and the bound there has been only in $L_F$. Please see the revised paper for details.
%\\
%
%\noindent ``\textit{2. There is much room for this paper to be better presented. There are also some obvious typos in the statements of theorems. The Equation 3.3 should be about $\gamma$ instead of $\lambda$. In Theorem 4.3, $Q^K$ does not need to appear.''}\\
%Answer:\\
%We have corrected them. Meanwhile, we have largely rewritten the paper and the paper went through a proofread process.\\
%
%\noindent ``\textit{3. In the paragraph after the Theorem 4.3, the authors seem to conclude that spectral clustering gives the exact recovery. If so, then maybe it can be stated as a theorem/corollary.''}\\
%Answer:\\
%Yes, you are right. Thank you very much for your suggestions. The consistency of the proposed method is discussed in subsection 4.2.2 where  Theorem 4.4 is added to explain the strong consistency of SCDRE under the challenging region. Moreover, Theorem 4.4 does imply exact recovery. Also, we would like to mention that Theorem 4.4 holds when the community number greater than or equal to 2, without any additional conditions on the edge probability.\\
%
%
%\noindent ``\textit{4. In Section 4.2.2, I am not sure if the truncation step is necessary. The supporting argument the authors give in Section 4.2.1 is not convincing.}''\\
%Answer:\\
%The truncation step on $\hat R_\zeta(i)$ in Section 4.2.1 is indeed no longer needed now and it has been removed in the revised version. In fact, we have realized that this is also one of the advantages of our method comparing to the recent SCORE and its variants. The details can be found in the subsection 4.2.3 which.\\
%
%\noindent ``\textit{5. In the statement of Theorem 4.1, referring $C_n$ a constant associated with $n'$ may cause confuse. There must be better ways to call it.}''\\
%Answer:\\
%For the statement of Theorem 4.1, the estimator of $K$, we have changed the notation $C_n$ in the estimator of $K$, which is a better expression in the regime $p_{ij}=O(\varphi_n n^{-1})$.



%%%%%%%%%%%%
%%%%%%%%%%%%
%%%%%%%%%%%%
%%%%%%%%%%%%
\end{document}
